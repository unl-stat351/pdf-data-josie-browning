---
title: "Lab: PDF Data"
author: "Josie Browning"
format: html
number-sections: true
number-depth: 2
---

::: callout
You can see the purpose of this assignment as well as the skills and knowledge you should be using and acquiring, in the [Transparency in Learning and Teaching (TILT)](tilt.qmd) document in this repository. The TILT document also contains a checklist for self-reflection that will provide some guidance on how the assignment will be graded.
:::

# Data Source

The University of Nebraska publishes the operating budgets by department on an annual basis on [the UN System business and finance office website](https://nebraska.edu/offices-policies/business-finance/budget-and-planning), with [an archive page that contains budgets from previous fiscal years](https://nebraska.edu/offices-policies/business-finance/budget-and-planning/archive).

With impending system-wide budget cuts, our goal is to extract salary data as well as job position data, to determine how much of the cost growth is attributable to growth in administrator salaries, faculty salaries, staff salaries, and other costs.

I have preemptively included 4 budget reports in this repository, spaced at 5 year intervals, but you are welcome to include more years if you would like to do so. The budget reports are lengthy (each report is between 1350 and 1500 pages, and that is just for city campus -- it does not include IANR, the law school, the dental college, etc.).

# Warming Up

## PDF Format

What type of PDF files are these? Based on the format, what would you have to do to process the PDFs and extract text and/or tabular information (in broad terms)?

> I believe these are text layered only. I am able to interact with all the text and can't find any sections that indicate an image layer existing. To process the PDF, I will use libraries pdftools. Depending on how exactly the data is nested, I will probably need to unlist and transform the data to get it into a tidy form.

------------------------------------------------------------------------

## PDF Structure

Take a look at the provided PDFs in your default PDF viewer (acrobat, reader, chrome, xPDF, etc). Do they have a consistent structure across years? Across departments? Make a list of at least 3-5 problems you expect to have to overcome if you scrape data from the PDFs. Include screenshots where it is relevant to do so (similar to Fig 34.4 in the textbook), and make sure your images are included using appropriate markdown syntax, captions, and hyperlinks. Discuss how you might overcome these problems and why the PDF format leads to processing challenges.

------------------------------------------------------------------------

1.  The first issue that stands out to me is how the text is structured in the PDF. For the most part, I believe the headers and number IDs are consistent, but there is an odd hierarchy and formatting in how the data is nested. Within "description", there are "faculty" and "non faculty", and within that there are other headers as well as the faculty names. These all should be their own variables but instead will likely all be read within the same column.

2.  I can see subtotals and totals in the "permanent budget" column, which will be an issue. These will appear as duplicates despite being shown here as separate observations. Using these PDFs for research or data collection needs to avoid having subtotals sandwiched in with all the other data. It seems like there is a high risk associated with causing a miscount or confusing number reporting situations.

3.  Another issue that will likely show up is the how the odd indentations, spaces, and page headers will be read into a dataframe. These could cause observations that should be within in the same variable to be separate. Overall, seems like a headache.

------------------------------------------------------------------------

![](images/Image%2010-7-25%20at%2012.02%20PM-01.jpeg)

![](images/IMG_6592-01.jpeg)

------------------------------------------------------------------------

## Plan of Attack

What strategy would you use to read in the budget data to minimize the amount of post-processing you need to do? Explain your reasoning.

------------------------------------------------------------------------

> Some of the things I want to address first is understanding how the data will be read in. Exploring the metadata could reveal some useful information on how the pdfs should be handled. The general formatting is what I think is going to be the biggest hurdle, which is expected with pdfs.
>
> Knowing that the indentations and odd spacing might cause issues in how columns are read, I'm not sure if there is anything I can do to minimize having to clean stuff up and address the spacing in the post-process. Anything that can be done so that the text data can be more easily read into a tabular structure should probably be done.

------------------------------------------------------------------------

## Acquiring Metadata

Use a PDF library to programmatically examine each PDF. Use a functional approach, and organize the metadata in a table, with one row per file. Do you notice any anomalies or unfamiliar metadata components which might be important? Propose a possible hypothesis for any anomalies you discover, and research/explain any unfamiliar terms in the metadata that you identified.

------------------------------------------------------------------------

```{r, message=FALSE, warning=FALSE}
library(tabulapdf)
library(pdftools)
library(purrr)
library(stringr)
library(dplyr)
library(tidyr)
library(xml2)
library(tibble)
library(readr)
library(ggplot2)
```

```{r}
files <- list.files(path = "pdf_files", pattern = ".pdf$", full.names = TRUE)

extract_meta <- function(file) {
    files_list <- pdf_info(file)
    
    meta_table <- as_tibble(files_list) %>%
        mutate(
            Author= keys[1],
            Creator = keys[2],
            Producer = keys[3],
            Title = keys[4]
        ) %>%
        unnest(everything()) %>%
        select(-keys) %>% #this is unique for each row, but we only need one row per file
        distinct()
    
    return(meta_table)
}

meta_table <- map_dfr(files, extract_meta)

text <- meta_table$metadata |>
  str_split("\n")

#head(text)
```

------------------------------------------------------------------------

> I notice that the meta data looks like xml format. I will need to explore this further to confirm if that's accurate. Looking at the nested structure, it looks like the metadata is contained within different separate files. The "key" variable has different locations(?) listed for each file, so figuring out how to access the data within these files will be the next challenge.

------------------------------------------------------------------------

## Anomalies and Strategy Adjustments

Considering what you discovered in the previous step, do you need to adjust your strategy for reading in the data? Why or why not? Investigate any differences in metadata values across files, and determine whether or not the variation(s) may pose problems for your analysis.

------------------------------------------------------------------------

> Yes, I believe I need to adjust how I go about reading the data because I know more about how it is stored now. There is another layer I need to access since the metadata is stored within the "key" variable.

```{r}
meta_table$linearized

meta_table$Creator

meta_table$Producer

meta_table$Title
```

------------------------------------------------------------------------

> It looks like there is some differences between the files, likely due to them being created over a long period of time and therefore requiring different version/types of programs. The most recent pdf uses Abode Acrobat instead of Acrobat Distiller or PScript5, which could cause issues. Another notable difference is that only two files are linearized (2015 and 2020). After some research I understand that linearized pdfs have internal structures that are designed to have pages load faster for viewing, versus all the pages loading first before they can be viewed. As of right now, I believe that this will not cause any issues in how r handles the pdfs.

------------------------------------------------------------------------

# Extracting the Text

Now that you've examined the metadata and prepared a strategy, let's see if we can extract the text from each budget report.

## Identify Relevant Pages

Develop a function that takes the path to a PDF file and identifies which pages have tabular salary data on them (e.g. get a range of pages with the salary information). Use your function to create a table with columns `file_name`, `page_start`, and `page_end`.

------------------------------------------------------------------------

```{r}
find_pages <- function(file) {
    txt <- pdf_text(file)
    txt_by_page <- map_chr(txt, ~ paste(., collapse = " "))
    salary_pages <- which(str_detect(txt_by_page, regex("salary|wage|position|fte|annual", ignore_case = TRUE)))
    tibble(
        file_name = basename(file),
        page_start = min(salary_pages),
        page_end = max(salary_pages)
    )
}
  
salary_pages <- map_dfr(files, find_pages)  
```

------------------------------------------------------------------------

## Read in Relevant Text

Develop a function `read_salary_data(file, start, end)` which will read in all of the salary data from the pages with tables, using the 2025-2026 salary report as a guide. You should not generalize to other years yet. Use the `pdf_text` function in `tabulapdf` (R) or the `read_pdf` function in the `tabula-py` package (python).

------------------------------------------------------------------------

```{r}
read_salary_data <- function(file, start, end) {
    pages <- pdf_text(file)
    target_pages <- pages[start:end]
  
    lines <- str_split(target_pages, "\n") |>
        flatten_chr() |>
        str_squish() |>
        discard(~ .x == "")
    
    lines <- lines[!str_detect(lines, regex("page|budget|department of|2025-2026", ignore_case = TRUE))]
  
    split_lines <- str_split(lines, "\\s{2,}")
    
    df <- tibble(text = unlist(split_lines))
}

file <- "pdf_files/unl-department-budget-2025-2026.pdf"

salary_data_2025 <- read_salary_data(file, start = 24, end = 27) #example pages

head(salary_data_2025, n = 20)

```

------------------------------------------------------------------------

## Plan your Approach

What processing steps would you use to get the text vectors from this function into a table? Make a detailed list of the necessary steps. Are there any steps you do not think will be consistently successful or generalizable? Is there information your steps sacrifice to read things in cleanly?

------------------------------------------------------------------------

> I got ahead of myself a little bit and already got the data into a table, which I did by turning the strings into digestible lines that can be formatted as rows in a table. That being said, the rows are very messy and have a lot of variables and information are packed into single cells. So to go about reformatted this data into a usable table, here are my next steps:
>
> 1.  May need to manually set what I want the headers to be
>
> 2.  Split up columns
>
>     1.  Delimit by semi-column, there will still be a lot of stuff to split up later
>
> 3.  Pivot wider, use set headers so that the variables are set up properly
>
> 4.  Delimit anything else that needs to be split up, probably by space with special conditions

------------------------------------------------------------------------

## Would Coordinates Help?

There are other functions in the tabula software which provide the coordinates of each piece of text. How might this make it easier to ensure tables are read in correctly?

------------------------------------------------------------------------

> My understanding of text coordinates is that they are the x-y position of a piece of text on a pdf page. So the tabula software would create two variables that could tell you the "location" of text on the page as it was originally meant to be displayed. This would be very helpful because this location-based reading would place text in a much more coherent order than how the pdf text is often read in using pdf-text or other pdftools function.

------------------------------------------------------------------------

## Explore Package Documentation

Find a function that will provide coordinates for each text component, and write out the steps you might use to convert this data into a clean tabular format. What challenges will you face?

------------------------------------------------------------------------

> The function that I think will work best here is pdf_data, which would give me a large list containing coordinates and other information for each page.
>
> 1.  Starting with a large list, I would convert this to a table using bind_rows
> 2.  Group by page number (I can narrow down the relavent pages by checking the pdfs)
> 3.  All the information we need will be in the "text" variable, so we need to reformat that variable
>     1.  Pick out preset headers, maybe a new T/F column for if the observation is a header
>     2.  Divide the other observations (not headers) by x-coordinate, which should theoretically create groups based on their visual indentation, so if header = false, then assign the observation to a new variable group based on different ranges of x-coordinates
> 4.  Other cleaning tasks to make sure the table is tidy and can handle the NAs that would probably be introduced
>
> A challenge with this will probably come with handling all the different lengths of variables. There also might be some manual work in setting up the expected headers that need to be identified, as well as establishing the x-coordinate ranges. My first try would be to see if equal intervals of x ranges works, but that may not work. This would definitely take some trial and error even just setting up a function to do it.

------------------------------------------------------------------------

## A Classical Problem

Using either approach, get the salary data for all individuals in the Classics department, across all 4 reports (this should require reading in about 2 pages from each report). Plot the salaries of each individual. Generate a second plot of the total budget for the classics department, split by faculty, administration (the chair), and staff (including student workers. What do you notice?

Your answer should address some of the following questions:

-   How has the budget for Classics changed over the last 15 years?
-   How has the proportional allocation of salaries to faculty, staff, and administration changed?
-   What do you think is driving that change?

Your plots must include appropriate titles and legends, and be well constructed using an appropriate mapping. Each plot should be accompanied by a 2-4 sentence description.

------------------------------------------------------------------------

```{r} 
data_list <- map(files, pdf_data)

#page check -> 104/105, 110/111, 110/111, 105/106
#I know this is not the most efficient way to do this, I'm tired
df1 <- bind_rows(data_list[1], .id = "page") %>%
    filter(page %in% c(104, 105)) %>%
    mutate(id = 1)
df2 <- bind_rows(data_list[3], .id = "page") %>%
    filter(page %in% c(110, 111)) %>%
    mutate(id = 2)
df3 <- bind_rows(data_list[4], .id = "page") %>%
    filter(page %in% c(110, 111)) %>%
    mutate(id = 3)
df4 <- bind_rows(data_list[5], .id = "page") %>%
    filter(page %in% c(105, 106)) %>%
    mutate(id = 4)

classics_pages <- bind_rows(df1, df2, df3, df4, .id = NULL)

#locate_areas(files[1], pages = 104)
#name/title column range: 210-385
#salary column range: 720-775
#y range to remove: 0-132

classics_lines <- classics_pages %>%
    filter((x >= 0 & x <= 386) | (x >= 720 & x <= 775),
           y >= 153) %>%
    group_by(id, page, y) %>% #group by line, or observations with the same y
    summarise(line = paste(text, collapse = " "), .groups = "drop")

classics_lines$y[141] <- 182 #manual change because the y line did not line up

titles <- c("Professor", "Assistant Professor of Practice", "Associate Professor of Practice", "Associate Professor", "Graduate Teaching Asst", "Chairperson", "Student Worker", "Assistant Professor", "Sr. Lecturer", "Staff Secy III")
#found these manually bc I couldn't figure it out
title_pattern <- paste(titles, collapse = "|")
     
classics_dpt <- classics_lines %>%
    group_by(id, page, y) %>%
    summarise(line = paste(line, collapse = " "),
            salary = as.numeric(gsub(",", "", 
                                     unlist(str_extract_all(line, "\\d{1,3}(?:,\\d{3})+")))), #pull out salaries
            .groups = "drop")

classics_dpt <- classics_dpt %>%
    mutate(line = str_remove_all(line, "\\d{1,3}(?:,\\d{3})+"),
           line = str_squish(line),
           title = str_extract(line, title_pattern), #extract titles if found
           line  = ifelse(!is.na(title), str_remove(line, fixed(title)), line)) %>% #remove titles from line obs
    filter(!str_detect(line, "\\d"), #remove obs that have id numbers 
           !str_detect(line, "^[A-Z\\s[:punct:]]+$")) #remove obs with only capitol letters

classics_dpt_clean <- classics_dpt %>%
    group_by(id) %>%
    mutate(
    total = str_detect(line, "\\bTOTAL\\b"), #true if "TOTAL" appears
    line = str_remove_all(line, "\\bTOTAL\\b"), #remove "TOTAL" wherever it appears
    line = str_squish(line)) %>% #clean up extra spaces
    group_by(line) %>%
    mutate(title = first(na.omit(title))) %>% #fill in NAs within each person group
    ungroup() %>%
    na.omit(title)

classics_dpt_clean <- classics_dpt_clean %>%
    group_by(line, id) %>%
    filter(!(any(total) & !total)) %>% #identify totals so there are not duplicate salaries
    ungroup() %>%
    select(-y) %>%
    distinct()

classics_dpt_clean
```

```{r, warning=FALSE}
avg_salaries <- classics_dpt_clean %>%
  group_by(line) %>%
  summarise(
    avg_salary = mean(salary, na.rm = TRUE),
    title = title) %>%
    distinct()

ggplot(avg_salaries, aes(x = reorder(line, avg_salary), y = avg_salary, fill = title)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Classics Department Salaries",
    x = "Faculty/Staff",
    y = "Salary",
    fill = "Title")
```

------------------------------------------------------------------------

> Overall observations

------------------------------------------------------------------------

## Quality Control

If you were to process the full set of faculty salary data across all departments, what quality control measures would you use to ensure your functions functioned as expected? Explain your answer and your reasoning.

------------------------------------------------------------------------

I noticed an issue with the first pdf (2010-2011) where the page numbers on the pdf display versus the preview software were not the same, which caused me to pull the wrong pages initially.
